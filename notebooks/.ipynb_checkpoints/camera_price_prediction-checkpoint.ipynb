{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67004d92",
   "metadata": {},
   "source": [
    "# Predicción de Precios de Cámaras Digitales: Un Proyecto de Machine Learning de Nivel Senior\n",
    "\n",
    "**Objetivo del Proyecto:** Desarrollar un modelo de Machine Learning robusto para predecir el precio (`Price`) de cámaras digitales basándose en sus características técnicas y fecha de lanzamiento.\n",
    "\n",
    "**Dataset:** `camera_dataset.csv`\n",
    "\n",
    "**Pasos del Proyecto:**\n",
    "1. Carga y Comprensión Inicial de Datos\n",
    "2. Análisis Exploratorio de Datos (EDA) Detallado\n",
    "3. Preprocesamiento e Ingeniería de Características Avanzada\n",
    "4. Desarrollo y Entrenamiento de Modelos de Regresión\n",
    "5. Evaluación Rigurosa y Selección del Modelo\n",
    "6. Optimización de Hiperparámetros Avanzada\n",
    "7. Interpretación del Modelo y Resultados Finales\n",
    "8. Interfaz de Predicción Interactiva\n",
    "9. Conclusiones y Próximos Pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da690d9",
   "metadata": {},
   "source": [
    "## 0.1. Importación de Librerías Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed4a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, IntSlider, FloatSlider, Dropdown, Textarea\n",
    "import warnings\n",
    "\n",
    "# Configuraciones adicionales\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a5242",
   "metadata": {},
   "source": [
    "## 1. Carga y Comprensión Inicial de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb920d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "try:\n",
    "    df_cameras = pd.read_csv(\"../data/camera_dataset.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: El archivo camera_dataset.csv no se encontró en la carpeta data/. Asegúrate de que la ruta es correcta.\")\n",
    "    # Intentar cargar desde la ruta de carga si es un entorno de ejecución diferente (ej. local vs. sandbox)\n",
    "    try:\n",
    "        df_cameras = pd.read_csv(\"camera_dataset.csv\") # Asumiendo que está en el mismo dir que el notebook si falla el anterior\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: No se pudo cargar el dataset desde ninguna ruta conocida.\")\n",
    "        df_cameras = pd.DataFrame() # Crear un DF vacío para evitar errores posteriores\n",
    "\n",
    "if not df_cameras.empty:\n",
    "    print(\"Dataset cargado exitosamente.\")\n",
    "    print(\"\n",
    "Primeras 5 filas:\")\n",
    "    display(df_cameras.head())\n",
    "    print(\"\n",
    "Últimas 5 filas:\")\n",
    "    display(df_cameras.tail())\n",
    "    print(f\"\n",
    "Dimensiones del dataset: {df_cameras.shape}\")\n",
    "    print(\"\n",
    "Información del dataset:\")\n",
    "    df_cameras.info()\n",
    "    print(\"\n",
    "Estadísticas descriptivas:\")\n",
    "    display(df_cameras.describe(include=\"all\"))\n",
    "    print(\"\n",
    "Conteo de valores nulos por columna:\")\n",
    "    display(df_cameras.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775174f",
   "metadata": {},
   "source": [
    "## 2. Análisis Exploratorio de Datos (EDA) Detallado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddde994",
   "metadata": {},
   "source": [
    "### 2.1. Análisis de la Variable Objetivo: `Price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eaccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_cameras.empty and 'Price' in df_cameras.columns:\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df_cameras['Price'], kde=True, bins=50)\n",
    "    plt.title(\"Distribución del Precio de las Cámaras\")\n",
    "    plt.xlabel(\"Precio\")\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=df_cameras['Price'])\n",
    "    plt.title(\"Boxplot del Precio de las Cámaras\")\n",
    "    plt.xlabel(\"Precio\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Considerar transformación logarítmica si está muy sesgado\n",
    "    df_cameras['Price_log'] = np.log1p(df_cameras['Price'])\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df_cameras['Price_log'], kde=True, bins=50)\n",
    "    plt.title(\"Distribución del Precio Log-Transformado\")\n",
    "    plt.xlabel(\"Log(Precio + 1)\")\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=df_cameras['Price_log'])\n",
    "    plt.title(\"Boxplot del Precio Log-Transformado\")\n",
    "    plt.xlabel(\"Log(Precio + 1)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"El DataFrame está vacío o la columna 'Price' no existe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba68ddf",
   "metadata": {},
   "source": [
    "### 2.2. Análisis Univariado de Características Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2272c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_cameras.empty:\n",
    "    numerical_features = df_cameras.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Excluir Price y Price_log ya analizados, y 'Release date' que se tratará también en bivariado\n",
    "    features_to_plot = [f for f in numerical_features if f not in ['Price', 'Price_log']]\n",
    "\n",
    "    for feature in features_to_plot:\n",
    "        if feature in df_cameras.columns:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.histplot(df_cameras[feature], kde=True, bins=30)\n",
    "            plt.title(f'Distribución de {feature}')\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            sns.boxplot(x=df_cameras[feature])\n",
    "            plt.title(f'Boxplot de {feature}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"El DataFrame está vacío.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8482c9",
   "metadata": {},
   "source": [
    "### 2.3. Análisis Bivariado y Multivariado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b962183",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_cameras.empty and 'Price' in df_cameras.columns:\n",
    "    # Precio vs. Características Numéricas\n",
    "    for feature in features_to_plot: # Usamos las mismas features que antes\n",
    "        if feature in df_cameras.columns and feature != 'Release date': # Release date se grafica aparte\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.scatterplot(x=df_cameras[feature], y=df_cameras['Price'])\n",
    "            plt.title(f'Precio vs. {feature}')\n",
    "            plt.xlabel(feature)\n",
    "            plt.ylabel(\"Precio\")\n",
    "            plt.show()\n",
    "\n",
    "    # Precio vs. Release Date\n",
    "    if 'Release date' in df_cameras.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(x=df_cameras['Release date'], y=df_cameras['Price'], marker='o', errorbar=None) # errorbar=None para limpiar\n",
    "        plt.title(\"Evolución del Precio Medio por Año de Lanzamiento\")\n",
    "        plt.xlabel(\"Año de Lanzamiento\")\n",
    "        plt.ylabel(\"Precio Medio\")\n",
    "        plt.show()\n",
    "\n",
    "    # Matriz de Correlación\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    # Seleccionar solo columnas numéricas para la correlación, incluyendo Price_log\n",
    "    corr_matrix = df_cameras[numerical_features].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5)\n",
    "    plt.title(\"Matriz de Correlación de Características Numéricas\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"El DataFrame está vacío o la columna 'Price' no existe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67246a54",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento e Ingeniería de Características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8609b71",
   "metadata": {},
   "source": [
    "### 3.1. Limpieza y Manejo de Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10768ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_cameras.empty:\n",
    "    print(\"Valores nulos antes de la imputación:\")\n",
    "    print(df_cameras.isnull().sum()[df_cameras.isnull().sum() > 0])\n",
    "\n",
    "    # Estrategias de imputación (ejemplos, ajustar según EDA más profundo)\n",
    "    # Para 'Macro focus range', 0 podría ser un valor razonable si NaN significa sin modo macro o foco infinito.\n",
    "    if 'Macro focus range' in df_cameras.columns: df_cameras['Macro focus range'].fillna(0, inplace=True)\n",
    "    # Para otras numéricas, usar mediana podría ser más robusto a outliers que la media.\n",
    "    cols_to_impute_median = ['Storage included', 'Weight (inc. batteries)', 'Dimensions']\n",
    "    for col in cols_to_impute_median:\n",
    "        if col in df_cameras.columns: df_cameras[col].fillna(df_cameras[col].median(), inplace=True)\n",
    "\n",
    "    print(\"\n",
    "Valores nulos después de la imputación:\")\n",
    "    print(df_cameras.isnull().sum()[df_cameras.isnull().sum() > 0])\n",
    "    if df_cameras.isnull().sum().sum() == 0:\n",
    "        print(\"\n",
    "Todos los valores nulos han sido tratados.\")\n",
    "else:\n",
    "    print(\"El DataFrame está vacío.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e5513",
   "metadata": {},
   "source": [
    "### 3.2. Ingeniería de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_cameras.empty and 'Release date' in df_cameras.columns:\n",
    "    # 1. Camera Age (respecto al año más reciente en el dataset)\n",
    "    max_year = df_cameras['Release date'].max()\n",
    "    df_cameras['Camera Age'] = max_year - df_cameras['Release date']\n",
    "\n",
    "    # 2. Zoom Ratio\n",
    "    if 'Zoom wide (W)' in df_cameras.columns and 'Zoom tele (T)' in df_cameras.columns:\n",
    "        # Evitar división por cero si Zoom wide (W) es 0. Si es 0, el ratio es indefinido o podría ser 1 (sin zoom óptico).\n",
    "        df_cameras['Zoom Ratio'] = np.where(df_cameras['Zoom wide (W)'] > 0, df_cameras['Zoom tele (T)'] / df_cameras['Zoom wide (W)'], 1)\n",
    "    else:\n",
    "        df_cameras['Zoom Ratio'] = 1 # Valor por defecto si las columnas no existen\n",
    "\n",
    "    # 3. Resolution Area (Max_resolution * Low_resolution) - Asumiendo que son dimensiones, aunque el EDA sugiere que son #pixeles.\n",
    "    # Si son #pixeles, Max_resolution es más directo. Si son dimensiones, el producto es área.\n",
    "    # El notebook de referencia lo calcula como producto, así que lo replicamos para consistencia, pero con cautela.\n",
    "    if 'Max resolution' in df_cameras.columns and 'Low resolution' in df_cameras.columns:\n",
    "        df_cameras['Resolution Area (MP)'] = (df_cameras['Max resolution'] * df_cameras['Low resolution']) / 1000000 # Convertir a Megapixeles\n",
    "    else:\n",
    "        df_cameras['Resolution Area (MP)'] = 0 # Valor por defecto\n",
    "\n",
    "    # 4. Weight to Dimension Ratio (proxy de densidad/robustez) - Cuidado con dimensiones = 0\n",
    "    if 'Weight (inc. batteries)' in df_cameras.columns and 'Dimensions' in df_cameras.columns:\n",
    "        df_cameras['Weight_Dim_Ratio'] = np.where(df_cameras['Dimensions'] > 0, df_cameras['Weight (inc. batteries)'] / df_cameras['Dimensions'], 0)\n",
    "    else:\n",
    "        df_cameras['Weight_Dim_Ratio'] = 0\n",
    "\n",
    "    print(\"\n",
    "Nuevas características creadas:\")\n",
    "    display(df_cameras[['Camera Age', 'Zoom Ratio', 'Resolution Area (MP)', 'Weight_Dim_Ratio']].head())\n",
    "else:\n",
    "    print(\"El DataFrame está vacío o 'Release date' no existe para crear 'Camera Age'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbf9bd",
   "metadata": {},
   "source": [
    "### 3.3. Selección Final de Características y División de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_cameras.empty and 'Price_log' in df_cameras.columns:\n",
    "    # Seleccionar características para el modelo. Excluir 'Model', 'Price' original, y quizás 'Low resolution' si 'Max resolution' y 'Resolution Area' son suficientes.\n",
    "    # También excluimos 'Release date' original si 'Camera Age' la reemplaza bien.\n",
    "    features = [\n",
    "        'Max resolution', 'Effective pixels', 'Zoom wide (W)', 'Zoom tele (T)', \n",
    "        'Normal focus range', 'Macro focus range', 'Storage included', \n",
    "        'Weight (inc. batteries)', 'Dimensions', 'Camera Age', 'Zoom Ratio', \n",
    "        'Resolution Area (MP)', 'Weight_Dim_Ratio'\n",
    "    ]\n",
    "    # Asegurarse de que todas las features seleccionadas existen en el df\n",
    "    features = [f for f in features if f in df_cameras.columns]\n",
    "\n",
    "    X = df_cameras[features]\n",
    "    y = df_cameras['Price_log'] # Usar el precio log-transformado como objetivo\n",
    "\n",
    "    print(f\"Características seleccionadas para X ({X.shape[1]}): {X.columns.tolist()}\")\n",
    "\n",
    "    # División en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Tamaño de X_train: {X_train.shape}\")\n",
    "    print(f\"Tamaño de X_test: {X_test.shape}\")\n",
    "else:\n",
    "    print(\"El DataFrame está vacío o 'Price_log' no se ha creado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e33d632",
   "metadata": {},
   "source": [
    "### 3.4. Pipeline de Preprocesamiento (Escalado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals():\n",
    "    # Solo necesitamos escalar las características numéricas, ya que no tenemos categóricas seleccionadas (aparte de 'Model' que fue excluida)\n",
    "    # Si tuviéramos categóricas, aquí iría el ColumnTransformer con OneHotEncoder, etc.\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Datos escalados. X_train_scaled shape:\", X_train_scaled.shape)\n",
    "else:\n",
    "    print(\"X_train no está definido. Ejecuta la celda anterior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7774e",
   "metadata": {},
   "source": [
    "## 4. Desarrollo y Entrenamiento de Modelos de Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ce168",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train_scaled' in locals():\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(random_state=42),\n",
    "        'Lasso Regression': Lasso(random_state=42),\n",
    "        'ElasticNet': ElasticNet(random_state=42),\n",
    "        'KNeighbors Regressor': KNeighborsRegressor(),\n",
    "        'Decision Tree Regressor': DecisionTreeRegressor(random_state=42),\n",
    "        'Random Forest Regressor': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42),\n",
    "        'XGBoost Regressor': xgb.XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror'),\n",
    "        'LightGBM Regressor': lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbosity=-1)\n",
    "        # 'SVR': SVR() # SVR puede ser lento, añadir si se desea\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        # Usar validación cruzada para una evaluación más robusta\n",
    "        # RMSE (neg_root_mean_squared_error) y R2 son buenas métricas\n",
    "        cv_rmse_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        cv_r2_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
    "        results[name] = {\n",
    "            'CV Mean RMSE': -np.mean(cv_rmse_scores),\n",
    "            'CV Std RMSE': np.std(cv_rmse_scores),\n",
    "            'CV Mean R2': np.mean(cv_r2_scores),\n",
    "            'CV Std R2': np.std(cv_r2_scores)\n",
    "        }\n",
    "        print(f\"Evaluado: {name}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results).T.sort_values(by='CV Mean RMSE', ascending=True)\n",
    "    print(\"\n",
    "Resultados de Validación Cruzada (ordenados por RMSE ascendente):\")\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"X_train_scaled no está definido. Ejecuta las celdas anteriores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551cc176",
   "metadata": {},
   "source": [
    "## 5. Evaluación y Selección del Modelo\n",
    "\n",
    "Basándonos en los resultados de la validación cruzada (especialmente `CV Mean RMSE` y `CV Mean R2`), podemos seleccionar los modelos más prometedores para la optimización de hiperparámetros. Modelos como LightGBM, XGBoost, Gradient Boosting y Random Forest suelen dar buenos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3403a0f",
   "metadata": {},
   "source": [
    "## 6. Optimización de Hiperparámetros Avanzada\n",
    "\n",
    "Seleccionaremos uno o dos de los mejores modelos para optimizar. Por ejemplo, LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471b2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train_scaled' in locals() and 'results_df' in locals() and not results_df.empty:\n",
    "    best_model_name = results_df.index[0] # El mejor según RMSE en CV\n",
    "    print(f\"Modelo seleccionado para optimización: {best_model_name}\")\n",
    "\n",
    "    # Ejemplo de optimización para LightGBM\n",
    "    if best_model_name == 'LightGBM Regressor':\n",
    "        param_grid_lgbm = {\n",
    "            'n_estimators': [100, 200, 500],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'num_leaves': [31, 50, 70],\n",
    "            'max_depth': [-1, 10, 20],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "            'subsample': [0.7, 0.8, 0.9]\n",
    "        }\n",
    "        lgbm_model = lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbosity=-1)\n",
    "        # Usar RandomizedSearchCV para una búsqueda más eficiente en espacios grandes\n",
    "        random_search_lgbm = RandomizedSearchCV(lgbm_model, param_distributions=param_grid_lgbm, \n",
    "                                              n_iter=50, cv=5, scoring='neg_root_mean_squared_error', \n",
    "                                              random_state=42, n_jobs=-1, verbose=1)\n",
    "        print(\"Iniciando RandomizedSearchCV para LightGBM...\")\n",
    "        random_search_lgbm.fit(X_train_scaled, y_train)\n",
    "        print(\"Mejores hiperparámetros para LightGBM:\", random_search_lgbm.best_params_)\n",
    "        print(f\"Mejor RMSE en CV (LightGBM optimizado): {-random_search_lgbm.best_score_:.4f}\")\n",
    "        optimized_model = random_search_lgbm.best_estimator_\n",
    "    elif best_model_name == 'XGBoost Regressor': # Ejemplo para XGBoost\n",
    "        param_grid_xgb = {\n",
    "            'n_estimators': [100, 200, 500],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "            'subsample': [0.7, 0.8, 0.9]\n",
    "        }\n",
    "        xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror')\n",
    "        random_search_xgb = RandomizedSearchCV(xgb_model, param_distributions=param_grid_xgb, \n",
    "                                             n_iter=50, cv=5, scoring='neg_root_mean_squared_error', \n",
    "                                             random_state=42, n_jobs=-1, verbose=1)\n",
    "        print(\"Iniciando RandomizedSearchCV para XGBoost...\")\n",
    "        random_search_xgb.fit(X_train_scaled, y_train)\n",
    "        print(\"Mejores hiperparámetros para XGBoost:\", random_search_xgb.best_params_)\n",
    "        print(f\"Mejor RMSE en CV (XGBoost optimizado): {-random_search_xgb.best_score_:.4f}\")\n",
    "        optimized_model = random_search_xgb.best_estimator_\n",
    "    else: # Si el mejor modelo no es LGBM o XGB, usar el modelo no optimizado de la lista original\n",
    "        print(f\"Optimizador no implementado para {best_model_name}, usando modelo base entrenado.\")\n",
    "        optimized_model = models[best_model_name] # Tomar el modelo ya instanciado\n",
    "        optimized_model.fit(X_train_scaled, y_train) # Re-entrenar en todo el train set\n",
    "\n",
    "    # Guardar el modelo optimizado y el scaler para la interfaz de predicción\n",
    "    final_model = optimized_model\n",
    "    final_scaler = scaler # El scaler ajustado en X_train\n",
    "    final_features = X_train.columns.tolist() # Guardar el orden de las features\n",
    "else:\n",
    "    print(\"Datos de entrenamiento o resultados de modelos no disponibles para optimización.\")\n",
    "    final_model = None; final_scaler = None; final_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a67332",
   "metadata": {},
   "source": [
    "## 7. Interpretación del Modelo y Resultados Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be369b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_model is not None and 'X_test_scaled' in locals():\n",
    "    # Predicciones en el conjunto de prueba\n",
    "    y_pred_log = final_model.predict(X_test_scaled)\n",
    "\n",
    "    # Revertir la transformación logarítmica para interpretar los resultados en la escala original del precio\n",
    "    y_test_original = np.expm1(y_test) # y_test es Price_log\n",
    "    y_pred_original = np.expm1(y_pred_log)\n",
    "\n",
    "    # Métricas en la escala original\n",
    "    rmse_original = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "    mae_original = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    r2_original = r2_score(y_test_original, y_pred_original)\n",
    "    mape_original = mean_absolute_percentage_error(y_test_original, y_pred_original)\n",
    "\n",
    "    print(f\"Resultados del Modelo Final ({type(final_model).__name__}) en el Conjunto de Prueba (Escala Original del Precio):\")\n",
    "    print(f\"  RMSE: ${rmse_original:.2f}\")\n",
    "    print(f\"  MAE: ${mae_original:.2f}\")\n",
    "    print(f\"  R²: {r2_original:.4f}\")\n",
    "    print(f\"  MAPE: {mape_original:.2%}\")\n",
    "\n",
    "    # Visualización de Predicciones vs. Reales\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test_original, y_pred_original, alpha=0.5, edgecolors='k')\n",
    "    plt.plot([y_test_original.min(), y_test_original.max()], [y_test_original.min(), y_test_original.max()], 'r--', lw=2)\n",
    "    plt.xlabel(\"Precios Reales\")\n",
    "    plt.ylabel(\"Precios Predichos\")\n",
    "    plt.title(\"Precios Reales vs. Predichos (Escala Original)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Importancia de Características (si el modelo lo soporta)\n",
    "    if hasattr(final_model, 'feature_importances_'):\n",
    "        importances = final_model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({'feature': final_features, 'importance': importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance_df.head(15)) # Top 15 features\n",
    "        plt.title(f'Importancia de Características ({type(final_model).__name__})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    elif hasattr(final_model, 'coef_'):\n",
    "        coefficients = final_model.coef_\n",
    "        feature_coeffs_df = pd.DataFrame({'feature': final_features, 'coefficient': coefficients})\n",
    "        feature_coeffs_df['abs_coefficient'] = np.abs(feature_coeffs_df['coefficient'])\n",
    "        feature_coeffs_df = feature_coeffs_df.sort_values(by='abs_coefficient', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='coefficient', y='feature', data=feature_coeffs_df.head(15)) # Top 15 features by abs_coefficient\n",
    "        plt.title(f'Coeficientes de Características ({type(final_model).__name__})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Modelo final o datos de prueba no disponibles para evaluación final.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18aa4ae",
   "metadata": {},
   "source": [
    "## 8. Interfaz de Predicción Interactiva\n",
    "\n",
    "Utiliza los siguientes controles para ingresar las características de una cámara y obtener una predicción de su precio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6940276",
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_model is not None and final_scaler is not None and final_features:\n",
    "    # Crear widgets para cada feature\n",
    "    # Usar df_cameras (antes de escalar y dividir) para obtener rangos razonables para sliders\n",
    "    # Si df_cameras no está disponible, usar valores por defecto genéricos\n",
    "    global_df_for_ranges = df_cameras if 'df_cameras' in locals() and not df_cameras.empty else pd.DataFrame(columns=final_features)\n",
    "\n",
    "    widget_dict = {}\n",
    "    for feature in final_features:\n",
    "        min_val = global_df_for_ranges[feature].min() if feature in global_df_for_ranges and not global_df_for_ranges[feature].empty else 0\n",
    "        max_val = global_df_for_ranges[feature].max() if feature in global_df_for_ranges and not global_df_for_ranges[feature].empty else 100\n",
    "        mean_val = global_df_for_ranges[feature].mean() if feature in global_df_for_ranges and not global_df_for_ranges[feature].empty else 50\n",
    "        step_val = (max_val - min_val) / 100 if max_val > min_val else 1\n",
    "\n",
    "        # Ajustar tipos de sliders y rangos\n",
    "        if global_df_for_ranges[feature].dtype == 'int64' or feature in ['Camera Age'] : # Asumir enteros para algunos\n",
    "             widget_dict[feature] = widgets.IntSlider(value=int(mean_val), min=int(min_val), max=int(max_val), step=1, description=feature, continuous_update=False, layout=widgets.Layout(width='90%'))\n",
    "        else: # Floats para el resto\n",
    "             widget_dict[feature] = widgets.FloatSlider(value=mean_val, min=min_val, max=max_val, step=step_val, description=feature, continuous_update=False, readout_format='.2f', layout=widgets.Layout(width='90%'))\n",
    "\n",
    "    # Output widget para mostrar la predicción\n",
    "    prediction_output = widgets.Output()\n",
    "\n",
    "    def predict_price(**kwargs):\n",
    "        with prediction_output:\n",
    "            prediction_output.clear_output()\n",
    "            try:\n",
    "                # Crear DataFrame con los inputs del usuario, asegurando el orden correcto de las columnas\n",
    "                input_data = pd.DataFrame([kwargs])[final_features]\n",
    "                # Escalar los datos de entrada\n",
    "                input_data_scaled = final_scaler.transform(input_data)\n",
    "                # Realizar la predicción (en escala logarítmica)\n",
    "                predicted_price_log = final_model.predict(input_data_scaled)[0]\n",
    "                # Revertir a la escala original\n",
    "                predicted_price_original = np.expm1(predicted_price_log)\n",
    "                print(f\"**Precio Predicho Estimado: ${predicted_price_original:.2f}**\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error durante la predicción: {e}\")\n",
    "\n",
    "    # Crear la interfaz interactiva\n",
    "    interactive_prediction = widgets.interactive_output(predict_price, widget_dict)\n",
    "    # Organizar los widgets verticalmente\n",
    "    ui_elements = [widget_dict[f] for f in final_features] # Lista de widgets\n",
    "    display(widgets.VBox(ui_elements), prediction_output)\n",
    "else:\n",
    "    print(\"El modelo final, el escalador o las características finales no están disponibles para la interfaz de predicción.\")\n",
    "    print(\"Asegúrate de que todas las celdas anteriores se hayan ejecutado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0b595",
   "metadata": {},
   "source": [
    "## 9. Conclusiones y Próximos Pasos\n",
    "\n",
    "En este proyecto, hemos desarrollado un pipeline de Machine Learning para predecir el precio de cámaras digitales. \n",
    "- Se realizó un análisis exploratorio detallado, preprocesamiento de datos e ingeniería de características.\n",
    "- Se entrenaron y evaluaron múltiples modelos de regresión, seleccionando el más performante (ej. LightGBM o XGBoost) tras la validación cruzada.\n",
    "- Se optimizaron los hiperparámetros del modelo seleccionado.\n",
    "- El modelo final fue evaluado en un conjunto de prueba, mostrando [Mencionar R², RMSE, MAE del test set aquí después de la ejecución].\n",
    "- Las características más influyentes en la predicción del precio fueron [Mencionar features importantes aquí].\n",
    "- Se implementó una interfaz interactiva para realizar predicciones con nuevas entradas.\n",
    "\n",
    "**Limitaciones:**\n",
    "- El dataset tiene un tamaño moderado y cubre un rango de años específico. Modelos más complejos podrían beneficiarse de más datos.\n",
    "- La característica 'Model' no se utilizó directamente para la predicción debido a su alta cardinalidad, aunque podría contener información de marca útil si se procesara.\n",
    "- La imputación de valores nulos se basó en estrategias generales; un conocimiento más profundo del dominio podría mejorarla.\n",
    "\n",
    "**Próximos Pasos Sugeridos:**\n",
    "- Recolectar datos más recientes y de una mayor variedad de fuentes.\n",
    "- Incorporar características adicionales como tipo de sensor, reviews de usuarios, o información de marca procesada.\n",
    "- Probar arquitecturas de Deep Learning si el dataset se expande significativamente.\n",
    "- Desplegar el modelo como una API web para un uso más amplio.\n",
    "- Realizar un análisis más profundo de la interpretabilidad del modelo (ej. SHAP values)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
